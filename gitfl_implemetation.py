# -*- coding: utf-8 -*-
"""GitFL-implemetation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NzVQpisOVpYqW2WP1RZqViqvFvq2yQpS

Source:

https://towardsdatascience.com/preserving-data-privacy-in-deep-learning-part-1-a04894f78029

# **1. Import all relevant packages**
"""

#### imports library

import math
import multiprocessing as mp
import sys
import time
from concurrent.futures import ProcessPoolExecutor
from functools import partial

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from sklearn.metrics import confusion_matrix
# from femnist_dataset import FEMNIST
from torch.utils.data.dataset import Dataset
from torchvision import datasets, transforms

torch.backends.cudnn.benchmark = True

"""print([
sys.version,
'GPU',
['is available', torch.cuda.is_available()],
['device count', torch.cuda.device_count()],
['current device', torch.cuda.current_device()],
['device', torch.cuda.device(0)],
['get device name', torch.cuda.get_device_name(0)],
['get_num_threads', torch.get_num_threads()],
'CPU',
['cpu_count', os.cpu_count()],
['is available', torch.cpu.is_available()],
['device count', torch.cpu.device_count()],
])"""

"""# **2. Loading and Dividing Datasets**

## 2.0 Plot
"""


def plot_distribution(dataloaders, num_client, num_label):
    plt.figure()
    ax = plt.gca()
    left = np.zeros(num_client)
    for idx, client in enumerate(dataloaders):
        label_count = np.zeros(num_label)
        for _, labels in client:
            for label in labels:
                label_count[label.item()] += 1
        for y, cnts in enumerate(label_count):
            ax.barh(idx, width=cnts, label=y, left=left[idx])
            left[idx] += cnts
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel('Number of Images')
    plt.ylabel('Number of Clients')
    plt.title('Data Distribution Among Clients')
    plt.savefig('DataPlot.png')


"""## 2.1 IID Datasets

### 2.1.1 CIFAR-10
"""


#### Loading and Dividing CIFAR-10
def iid_cifar10(num_clients, batch_size):
    # Image augmentation
    transform_train = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading CIFAR10 using torchvision.datasets
    traindata = datasets.CIFAR10('./data/cifar10', train=True, download=True,
                                 transform=transform_train)

    # Dividing the training data into num_clients, with each client having equal number of images
    traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in
                                                                range(num_clients)])

    # Creating a pytorch loader for a Deep Learning model
    train_data = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]

    # Draw a Plot
    plot_distribution(train_data, num_clients, len(traindata.classes))

    # Normalizing the test images
    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading the test images and thus converting them into a test_loader
    test_data = torch.utils.data.DataLoader(datasets.CIFAR10('./data/cifar10', train=False, transform=transform_test),
                                            batch_size=batch_size, shuffle=True)

    return train_data, test_data


"""### 2.1.2 CIFAR-100"""


#### Loading and Dividing CIFAR-100
def iid_cifar100(num_clients, batch_size):
    # Image augmentation
    transform_train = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading CIFAR100
    traindata = datasets.CIFAR100('./data/cifar100', train=True, download=True, transform=transform_train)

    # Dividing the training data into num_clients, with each client having equal number of images
    traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in
                                                                range(num_clients)])

    # Creating a pytorch loader for a Deep Learning model
    train_data = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]

    # Draw a Plot
    plot_distribution(train_data, num_clients, len(traindata.classes))

    # Normalizing the test images
    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading the test images and thus converting them into a test_loader
    test_data = torch.utils.data.DataLoader(datasets.CIFAR100('./data/cifar100', train=False, transform=transform_test),
                                            batch_size=batch_size, shuffle=True)

    return train_data, test_data


"""### 2.1.3 EMNIST"""


#### Loading and Dividing EMNIST
def iid_emnist(num_clients, batch_size):
    # Image augmentation
    transform_train = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading EMNIST using torchvision.datasets
    traindata = datasets.EMNIST('./data/emnist', train=True, download=True, transform=transform_train)

    # Dividing the training data into num_clients, with each client having equal number of images
    traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in
                                                                range(num_clients)])

    # Creating a pytorch loader for a Deep Learning model
    train_data = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]

    # Draw a Plot
    plot_distribution(train_data, num_clients, len(traindata.classes))

    # Normalizing the test images
    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading the test images and thus converting them into a test_loader
    test_data = torch.utils.data.DataLoader(datasets.EMNIST('./data/emnist', train=False, transform=transform_test),
                                            batch_size=batch_size, shuffle=True)

    return train_data, test_data


"""## 2.2 Non-IID Datasets

### 2.2.1 CIFAR-10
"""


#### Loading and Dividing CIFAR-10
def non_iid_cifar10(num_clients, batch_size, alpha):
    # Image augmentation
    transform_train = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading CIFAR10 using torchvision.datasets
    traindata = datasets.CIFAR10('./data/cifar10', train=True, download=True,
                                 transform=transform_train)

    # Get the targets and count the number of classes
    targets = np.array(traindata.targets)
    num_classes = np.max(targets) + 1

    # Create a list to hold data indices for each client
    client_dict = {i: [] for i in range(num_clients)}

    # For each class, split the data across the clients using Dirichlet distribution
    for k in range(num_classes):
        idx_k = np.where(targets == k)[0]
        np.random.shuffle(idx_k)
        proportions = np.random.dirichlet(np.repeat(alpha, num_clients))
        # Get the number of samples for each client
        num_samples_per_client = np.round(proportions * len(idx_k)).astype(int)
        # Correct any rounding errors
        num_samples_per_client[-1] = len(idx_k) - np.sum(num_samples_per_client[:-1])

        start = 0
        for i in range(num_clients):
            size = num_samples_per_client[i]
            client_dict[i].extend(idx_k[start:start + size])
            start += size

    # Create a data loader for each client
    train_data = [torch.utils.data.DataLoader(torch.utils.data.Subset(traindata, client_dict[i]), batch_size=batch_size,
                                              shuffle=True) for i in range(num_clients)]

    # Draw a Plot
    plot_distribution(train_data, num_clients, len(traindata.classes))

    # Normalizing the test images
    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading the test images and thus converting them into a test_loader
    test_data = torch.utils.data.DataLoader(datasets.CIFAR10('./data/cifar10', train=False, transform=transform_test),
                                            batch_size=batch_size, shuffle=True)

    return train_data, test_data


"""### 2.2.2 CIFAR-100"""


#### Loading and Dividing CIFAR-100
def non_iid_cifar100(num_clients, batch_size, alpha):
    # Image augmentation
    transform_train = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading CIFAR100 using torchvision.datasets
    traindata = datasets.CIFAR100('./data/cifar100', train=True, download=True,
                                  transform=transform_train)

    # Get the targets and count the number of classes
    targets = np.array(traindata.targets)
    num_classes = np.max(targets) + 1

    # Create a list to hold data indices for each client
    client_dict = {i: [] for i in range(num_clients)}

    # For each class, split the data across the clients using Dirichlet distribution
    for k in range(num_classes):
        idx_k = np.where(targets == k)[0]
        np.random.shuffle(idx_k)
        proportions = np.random.dirichlet(np.repeat(alpha, num_clients))
        # Get the number of samples for each client
        num_samples_per_client = np.round(proportions * len(idx_k)).astype(int)
        # Correct any rounding errors
        num_samples_per_client[-1] = len(idx_k) - np.sum(num_samples_per_client[:-1])

        start = 0
        for i in range(num_clients):
            size = num_samples_per_client[i]
            client_dict[i].extend(idx_k[start:start + size])
            start += size

    # Create a data loader for each client
    train_data = [torch.utils.data.DataLoader(torch.utils.data.Subset(traindata, client_dict[i]), batch_size=batch_size,
                                              shuffle=True) for i in range(num_clients)]

    # Draw a Plot
    plot_distribution(train_data, num_clients, len(traindata.classes))

    # Normalizing the test images
    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading the test images and thus converting them into a test_loader
    test_data = torch.utils.data.DataLoader(datasets.CIFAR100('./data/cifar100', train=False, transform=transform_test),
                                            batch_size=batch_size, shuffle=True)

    return train_data, test_data


"""### 2.2.3 EMNIST"""


#### Loading and Dividing EMNIST
def non_iid_emnist(num_clients, batch_size, alpha):
    # Image augmentation
    transform_train = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading EMNIST using torchvision.datasets
    traindata = datasets.EMNIST('./data/emnist', train=True, download=True,
                                transform=transform_train)

    # Get the targets and count the number of classes
    targets = np.array(traindata.targets)
    num_classes = np.max(targets) + 1

    # Create a list to hold data indices for each client
    client_dict = {i: [] for i in range(num_clients)}

    # For each class, split the data across the clients using Dirichlet distribution
    for k in range(num_classes):
        idx_k = np.where(targets == k)[0]
        np.random.shuffle(idx_k)
        proportions = np.random.dirichlet(np.repeat(alpha, num_clients))
        # Get the number of samples for each client
        num_samples_per_client = np.round(proportions * len(idx_k)).astype(int)
        # Correct any rounding errors
        num_samples_per_client[-1] = len(idx_k) - np.sum(num_samples_per_client[:-1])

        start = 0
        for i in range(num_clients):
            size = num_samples_per_client[i]
            client_dict[i].extend(idx_k[start:start + size])
            start += size

    # Create a data loader for each client
    train_data = [torch.utils.data.DataLoader(torch.utils.data.Subset(traindata, client_dict[i]), batch_size=batch_size,
                                              shuffle=True) for i in range(num_clients)]

    # Draw a Plot
    plot_distribution(train_data, num_clients, len(traindata.classes))

    # Normalizing the test images
    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    # Loading the test images and thus converting them into a test_loader
    test_data = torch.utils.data.DataLoader(datasets.EMNIST('./data/emnist', train=False, transform=transform_test),
                                            batch_size=batch_size, shuffle=True)

    return train_data, test_data


"""# **3. GitFL - Adaptive Asynchronous Federated Learning**

### 3.1 Merging
"""


#### Asynchronous Federated Learning
def merging(global_model, client_models, versions, global_opt, opt):
    # Create Dataframe with client_models, optimizer, versions
    df = pd.DataFrame(list(zip(client_models, opt, versions)),
                      columns=['client_models', 'optimizer', 'versions'])

    # boxplots calculations and Remove Outlier Versions
    df = df.sort_values(by='versions')

    q1 = df['versions'].quantile(0.25)
    q3 = df['versions'].quantile(0.75)
    iqr = q3 - q1
    qmin = q1 - (1.5 * iqr)
    qmax = q3 + (1.5 * iqr)
    mini = df['versions'].quantile(0)
    maxi = df['versions'].quantile(1)

    if mini > qmin:
        minimum = mini
    else:
        minimum = qmin

    if maxi <= qmax:
        maximum = maxi
    else:
        maximum = qmax

    for i, row in df['versions'].items():
        if row < minimum or maximum < row:
            df = df.drop(index=i)

    # Update Parameter and Merge & Average Layers Weight by Ensemble Learning
    parameter_state_dict = {}
    for key in global_model.state_dict():
        x = 0
        for _, client in df.iterrows():
            if client['versions'] == 0:
                # print(f"Warning: version for client is zero")
                pass
            elif torch.isnan(client['client_models'].state_dict()[key]).any():
                # print(f"Warning: NaN detected in parameters for client")
                pass
            else:
                x += client['client_models'].state_dict()[key] * client['versions']

        try:
            parameter_state_dict[key] = x / sum(df['versions'])
        except ZeroDivisionError:
            # print("model exception")
            parameter_state_dict[key] = global_model.state_dict()[key]
    global_model.load_state_dict(parameter_state_dict)

    # Update Hyper Parameter and Merge & Average Optimization Parameter
    for key in ['lr', 'momentum', 'dampening', 'weight_decay']:
        x = 0
        for _, client in df.iterrows():
            if client['versions'] == 0:
                # print(f"Warning: version for client is zero")
                pass
            else:
                x += client['optimizer'].state_dict()['param_groups'][0][key] * client['versions']
        try:
            global_opt.state_dict()['param_groups'][0][key] = x / sum(df['versions'])
        except ZeroDivisionError:
            # print("opt exception")
            global_opt.state_dict()['param_groups'][0][key] = global_opt.state_dict()['param_groups'][0][key]

    return global_model, global_opt


"""### 3.2 Version Control"""


def version_control(versions):
    sum_versions = sum(versions)
    v_ctrl = []
    for i in versions:
        v_ctrl.append((i - sum_versions) / len(versions))
    return v_ctrl


"""### 3.3 Client Selection"""


def client_selection(Tc, Tt, v_ctrl, active_client):
    # Calculate Version Reward (Rv)
    sum_Tt = sum(Tt) / len(Tt)
    max_Tt = max(Tt)
    sum_max_Tt = []
    for i in Tt:
        x = (i - sum_Tt)
        try:
            x /= max_Tt
        except ZeroDivisionError:
            x = 0
        sum_max_Tt.append(x)
    Rv = [a * b for a, b in zip(sum_max_Tt, v_ctrl)]

    # Calculate Curiosity Reward (Rc)
    Rc = []
    for i in Tc:
        try:
            Rc.append(1 / math.sqrt(i))
        except ZeroDivisionError:
            Rc.append(0)

    # Calculate Version-Curiosity Reward (R)
    R_sum = [a + b for a, b in zip(Rv, Rc)]
    R = [max(0, r) for r in R_sum]

    # Calculate Price (P)
    sum_R = sum(R)
    P = []
    for i in R:
        try:
            P.append(i / sum_R)
        except ZeroDivisionError:
            P.append(0)

    # Select High Price Client then Not Active
    maxi = 0
    client_index = None
    for i, value in enumerate(P):
        if value >= maxi:
            if i not in active_client:
                maxi = value
                client_index = i
    active_client.append(client_index)

    return client_index


"""### 3.4 Model Pull"""


def model_pull(global_model, client_model, global_opt, opt, v_ctrl):
    y_max = max(10 + v_ctrl, 2)

    # Aggregation Parameter of Client Model and Global Model
    parameter_state_dict = {}
    for key in global_model.state_dict():
        x = y_max * client_model.state_dict()[key] + global_model.state_dict()[key]
        parameter_state_dict[key] = x / (y_max + 1)
    client_model.load_state_dict(parameter_state_dict)

    # Aggregation Optimization Hyper Parameter of Client Model and Global Model
    for key in ['lr', 'momentum', 'dampening', 'weight_decay']:
        x = y_max * opt.state_dict()['param_groups'][0][key] + global_opt.state_dict()['param_groups'][0][key]
        opt.state_dict()['param_groups'][0][key] = x / (y_max + 1)

    return client_model, opt


"""### 3.5 Local Training"""


def local_training(model_client, opt_client, train_loader, epoch=5):
    """
    This function updates/trains client model on client data
    """
    model_client.train()
    for e in range(epoch):
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.cuda(), target.cuda()
            opt_client.zero_grad()
            output = model_client(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            opt_client.step()

    return model_client, opt_client


"""### 3.6 Test Function"""


def test(global_model, test_loader, client_idx, rnd):
    # Ensure the model is in evaluation mode
    global_model.eval()

    # Lists to store actual and predicted labels
    y_test = []
    y_pred = []

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.cuda(), target.cuda()

            # Assuming your data is on the same device as your model
            output = global_model(data)

            # Get the predicted classes
            pred = output.argmax(dim=1, keepdim=True)

            y_test.extend(target.tolist())
            y_pred.extend(pred.reshape(-1).tolist())

    # Calculate confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Calculate accuracy from confusion matrix
    accuracy = np.trace(cm) / float(np.sum(cm))

    print("local client: {} - Round: {} - Test Accuracy: {}\n".format(client_idx, rnd, accuracy))
    with open("acc_output.csv", "a") as file:
        file.write(str(client_idx) + ',\t' + str(rnd) + ',\t' + str(accuracy) + '\n')


"""### 3.7 Main Route for GitFL"""


def train(local_client, num_rounds, global_model, client_models, global_opt, opt, versions, Tc, Tt, active_client,
          epochs, train_loader, test_loader):
    # print('local client: {} Train Started'.format(local_client))
    for rnd in range(int(num_rounds)):
        print('local client: {} Round: {}   Train Started'.format(local_client, rnd))
        # Merging
        global_model, global_opt = merging(global_model, client_models, versions, global_opt, opt)
        # print('Merging Done')
        # Version Control
        v_ctrl = version_control(versions)
        # print('Version Control Done')
        # Client Selection
        client_idx = client_selection(Tc, Tt, v_ctrl, active_client)
        # print('Client Selection Done')
        # Model Pulling
        model_client, opt_client = model_pull(global_model, client_models[client_idx], global_opt, opt[client_idx],
                                              v_ctrl[client_idx])
        # print('Model Pull Done')
        # Model Training
        model_client, opt_client = local_training(model_client, opt_client, train_loader[client_idx], epochs)
        # print('local Training Done')
        # Update Versions
        versions[client_idx] = versions[client_idx] + 1
        # Model Pushing - update local model
        client_models[client_idx], opt[client_idx] = model_client, opt_client
        # Update Tc
        Tc[client_idx] = Tc[client_idx] + 1
        # Update Tt
        Tt[client_idx] = Tt[client_idx] + 1
        # Test Accuracy
        test(global_model, test_loader, local_client, rnd)
        # Remove client from active client
        print('local Client: {} Round: {}   Version: {}'.format(local_client, rnd, versions))
        print('local Client: {} Round: {}   Active Clients: {}\n'.format(local_client, rnd, active_client))
        active_client.remove(client_idx)


"""# **4. Asynchronous Parallel Training**

## 4.1 Set Hyperparameters
"""

#### Set Hyperparameters
num_clients = 80
num_selected = 8
num_rounds = 13  # another round test is 200
epochs = 5
batch_size = 50

"""## 4.2 Create CNN Neural Network"""


#### Create CNN Neural Network
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = nn.functional.relu(x)
        x = nn.functional.max_pool2d(x, 2)
        x = self.conv2(x)
        x = nn.functional.relu(x)
        x = nn.functional.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = nn.functional.log_softmax(x, dim=1)
        return output


"""## 4.3 Parallel Processing"""


#### Initialize Model
def process(model, train_loader, test_loader):
    manager = mp.Manager()

    # Global Model
    global_model = model.cuda()
    global_model.share_memory()
    #global_model = manager.dict(global_model.state_dict())

    # Client Models
    client_models = [model.cuda() for _ in range(num_clients)]
    for model in client_models:
        model.load_state_dict(global_model.state_dict())  # initial synchronizing with global model
        model.share_memory()
    #client_models = [manager.dict(manager.dict(model.state_dict()) for model in client_models)]

    # Optimizers
    global_opt = optim.SGD(global_model.parameters(), lr=0.01, momentum=0.5)
    #global_opt = manager.dict(global_opt.state_dict())
    opt = [optim.SGD(model.parameters(), lr=0.01, momentum=0.5) for model in client_models]
    #opt = [manager.dict(o.state_dict()) for o in opt]

    # Version Controls Lists for Client Selection
    versions = manager.list(np.zeros(num_clients).tolist())
    Tc = manager.list(np.zeros(num_clients).tolist())
    Tt = manager.list(np.zeros(num_clients).tolist())
    active_client = manager.list()

    # Start Timer
    start_time = time.mktime(time.gmtime(time.time()))

    # Create a new function that has some parameters fixed
    train_partial = partial(train, num_rounds=num_rounds, global_model=global_model, client_models=client_models,
                            global_opt=global_opt, opt=opt, versions=versions, Tc=Tc, Tt=Tt,
                            active_client=active_client,
                            epochs=epochs, train_loader=train_loader, test_loader=test_loader)

    #### Asynchronous Parallel Processing
    with ProcessPoolExecutor(max_workers=num_selected) as executor:
        # Assign Function to each Processor Core
        executor.map(train_partial, range(num_selected))
        # can use submit (e.g.) future = [executor.submit(train_partial, i) for i in range(num_selected)]

    # Calculate Delta Time
    delta_time = int(time.mktime(time.gmtime(time.time())) - start_time)
    print('delta-time: ', delta_time)

    # Final Merging
    global_model, _ = merging(global_model, client_models, versions, global_opt, opt)

    # Final Test
    test(global_model, test_loader, client_idx=101, rnd=101)

    # Calculate Communication Overhead
    data_size = int(sys.getsizeof(pd.DataFrame(train_loader))) + int(sys.getsizeof(pd.DataFrame(test_loader)))
    model_size = int(sys.getsizeof(torch.save(global_model.state_dict(), buf)))
    comm_overhead = num_rounds * model_size * num_clients * data_size
    print('communication overhead: ', comm_overhead)


if __name__ == '__main__':
    train_loader, test_loader = non_iid_cifar10(num_clients, batch_size, 0.1)
    process(torchvision.models.vgg16(), train_loader, test_loader)
